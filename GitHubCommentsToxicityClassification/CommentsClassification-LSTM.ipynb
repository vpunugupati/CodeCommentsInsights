{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Bidirectional,LSTM,RepeatVector,TimeDistributed,Activation\n",
    "from keras.layers import BatchNormalization, Flatten, Conv1D, MaxPooling1D,GlobalMaxPool1D\n",
    "from keras.layers import Dropout,SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import Model, model_from_json\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(159571, 8)\n"
    }
   ],
   "source": [
    "# read in the data\n",
    "df_train = pd.read_csv('TrainTestData/ToxicCommentsTrainData.csv')\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['comment_text'] = df_train['comment_text'].fillna(\"unknown\")\n",
    "df_train['comment_text'] = df_train['comment_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                      id                                       comment_text  \\\n159561  ffd2e85b07b3c7e4  \"\\nno he did not, read it again (i would have ...   \n159562  ffd72e9766c09c97  \"\\n auto guides and the motoring press are not...   \n159563  ffe029a7c79dc7fe  \"\\nplease identify what part of blp applies be...   \n159564  ffe897e7f7182c90  catalan independentism is the social movement ...   \n159565  ffe8b9316245be30  the numbers in parentheses are the additional ...   \n159566  ffe987279560d7ff  \":::::and for the second time of asking, when ...   \n159567  ffea4adeee384e90  you should be ashamed of yourself \\n\\nthat is ...   \n159568  ffee36eab5c267c9  spitzer \\n\\numm, theres no actual article for ...   \n159569  fff125370e4aaaf3  and it looks like it was actually you who put ...   \n159570  fff46fc426af1f9a  \"\\nand ... i really don't think you understand...   \n\n        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n159561      0             0        0       0       0              0  \n159562      0             0        0       0       0              0  \n159563      0             0        0       0       0              0  \n159564      0             0        0       0       0              0  \n159565      0             0        0       0       0              0  \n159566      0             0        0       0       0              0  \n159567      0             0        0       0       0              0  \n159568      0             0        0       0       0              0  \n159569      0             0        0       0       0              0  \n159570      0             0        0       0       0              0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>159561</th>\n      <td>ffd2e85b07b3c7e4</td>\n      <td>\"\\nno he did not, read it again (i would have ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159562</th>\n      <td>ffd72e9766c09c97</td>\n      <td>\"\\n auto guides and the motoring press are not...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159563</th>\n      <td>ffe029a7c79dc7fe</td>\n      <td>\"\\nplease identify what part of blp applies be...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159564</th>\n      <td>ffe897e7f7182c90</td>\n      <td>catalan independentism is the social movement ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159565</th>\n      <td>ffe8b9316245be30</td>\n      <td>the numbers in parentheses are the additional ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159566</th>\n      <td>ffe987279560d7ff</td>\n      <td>\":::::and for the second time of asking, when ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159567</th>\n      <td>ffea4adeee384e90</td>\n      <td>you should be ashamed of yourself \\n\\nthat is ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159568</th>\n      <td>ffee36eab5c267c9</td>\n      <td>spitzer \\n\\numm, theres no actual article for ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159569</th>\n      <td>fff125370e4aaaf3</td>\n      <td>and it looks like it was actually you who put ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159570</th>\n      <td>fff46fc426af1f9a</td>\n      <td>\"\\nand ... i really don't think you understand...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df_train.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(159571, 150)\n"
    }
   ],
   "source": [
    "# define text data\n",
    "docs_train = df_train['comment_text'].astype(str)\n",
    "\n",
    "# initialize the tokenizer\n",
    "t = Tokenizer(num_words=200000)\n",
    "t.fit_on_texts(docs_train)\n",
    "with open('TrainTestData/SavedTokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(t, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the text data\n",
    "encoded_docs_train = t.texts_to_sequences(docs_train)\n",
    "# pad the vectors to create uniform length\n",
    "df_train_padded = pad_sequences(encoded_docs_train, maxlen=150, padding='post')\n",
    "print(df_train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loaded 2196016 word vectors.\n"
    }
   ],
   "source": [
    "# load the glove840B embedding into memory after downloading and unzippping\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open('Glove/glove.840B.300d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in f:\n",
    "\tvalues = line.split(' ')\n",
    "\tword = values[0]\n",
    "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# create a weight matrix\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN Create LSTM model\n",
    "#Best performing model-lr=0.0003\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], \n",
    "                  input_length=150, trainable=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add((Bidirectional(LSTM(50,return_sequences=True))))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(70, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(6, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 150, 300)          63101100  \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 150, 300)          0         \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 150, 100)          140400    \n_________________________________________________________________\nglobal_max_pooling1d_1 (Glob (None, 100)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 70)                7070      \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 70)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 6)                 426       \n=================================================================\nTotal params: 63,248,996\nTrainable params: 147,896\nNon-trainable params: 63,101,100\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\venka\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\n"
    }
   ],
   "source": [
    "# compile the model\n",
    "Adam_opt = Adam(lr=0.0003, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.000015)\n",
    "model.compile(optimizer = Adam_opt, loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n",
    "# serialize model to json\n",
    "json_model = model.to_json()\n",
    "#save the model architecture to JSON file\n",
    "with open('ModelData/LSTMModel.json', 'w') as json_file:\n",
    "    json_file.write(json_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define X and y\n",
    "X = df_train_padded\n",
    "y = df_train[list_classes].values\n",
    "#Split Training data into training data and validation data\n",
    "X_train, X_eval, y_train ,y_eval = train_test_split(X, y,test_size=0.05,shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\venka\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nTrain on 151592 samples, validate on 7979 samples\nEpoch 1/40\n151592/151592 [==============================] - 343s 2ms/step - loss: 0.0986 - auc: 0.7967 - val_loss: 0.0508 - val_auc: 0.9278\nEpoch 2/40\n151592/151592 [==============================] - 405s 3ms/step - loss: 0.0525 - auc: 0.9470 - val_loss: 0.0477 - val_auc: 0.9580\nEpoch 3/40\n151592/151592 [==============================] - 372s 2ms/step - loss: 0.0491 - auc: 0.9634 - val_loss: 0.0458 - val_auc: 0.9673\nEpoch 4/40\n151592/151592 [==============================] - 327s 2ms/step - loss: 0.0474 - auc: 0.9697 - val_loss: 0.0451 - val_auc: 0.9718\nEpoch 5/40\n151592/151592 [==============================] - 311s 2ms/step - loss: 0.0457 - auc: 0.9734 - val_loss: 0.0440 - val_auc: 0.9747\nEpoch 6/40\n151592/151592 [==============================] - 330s 2ms/step - loss: 0.0447 - auc: 0.9757 - val_loss: 0.0433 - val_auc: 0.9767\nEpoch 7/40\n151592/151592 [==============================] - 303s 2ms/step - loss: 0.0436 - auc: 0.9774 - val_loss: 0.0433 - val_auc: 0.9781\nEpoch 8/40\n151592/151592 [==============================] - 285s 2ms/step - loss: 0.0429 - auc: 0.9787 - val_loss: 0.0423 - val_auc: 0.9793\nEpoch 9/40\n151592/151592 [==============================] - 278s 2ms/step - loss: 0.0422 - auc: 0.9798 - val_loss: 0.0423 - val_auc: 0.9802\nEpoch 10/40\n151592/151592 [==============================] - 275s 2ms/step - loss: 0.0412 - auc: 0.9806 - val_loss: 0.0423 - val_auc: 0.9810\nEpoch 11/40\n151592/151592 [==============================] - 275s 2ms/step - loss: 0.0407 - auc: 0.9813 - val_loss: 0.0424 - val_auc: 0.9816\nEpoch 12/40\n151592/151592 [==============================] - 273s 2ms/step - loss: 0.0399 - auc: 0.9820 - val_loss: 0.0414 - val_auc: 0.9823\nEpoch 13/40\n151592/151592 [==============================] - 455s 3ms/step - loss: 0.0393 - auc: 0.9825 - val_loss: 0.0412 - val_auc: 0.9828\nEpoch 14/40\n151592/151592 [==============================] - 336s 2ms/step - loss: 0.0389 - auc: 0.9830 - val_loss: 0.0411 - val_auc: 0.9833\nEpoch 15/40\n151592/151592 [==============================] - 359s 2ms/step - loss: 0.0382 - auc: 0.9835 - val_loss: 0.0414 - val_auc: 0.9837\nEpoch 16/40\n151592/151592 [==============================] - 354s 2ms/step - loss: 0.0378 - auc: 0.9839 - val_loss: 0.0414 - val_auc: 0.9841\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x1d0f7abb808>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, mode='min',min_delta=0.0005)\n",
    "save_best = ModelCheckpoint('ModelData/LSTMModelWeights.hdf', save_best_only=True, \n",
    "                           monitor='val_auc', mode='max')\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_eval, y_eval),\n",
    "                    epochs=40, verbose=1,callbacks=[early_stopping,save_best],batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}